{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import config   \n",
    "from ipywidgets import widgets\n",
    "from modules.corpus import Corpus\n",
    "from program import setup_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(path, 'data')\n",
    "collection = list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Reddit data \n",
    "Please run following cell, if you want to change key word run again after make changes in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e55b5e8fa1a4748b129dac51be1d4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='MachineLearning')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subreddit = widgets.Text(value='MachineLearning')\n",
    "display(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit --------------------\n",
      "GETTING REDDIT DATA\n",
      "<modules.reddit_api.RedditApi object at 0x15fad2860>\n",
      "[=============================================     ] 90%  Reddit process"
     ]
    }
   ],
   "source": [
    "# RUN THIS AFTER THE SUBREDDIT SELECTION\n",
    "collection += setup_process(\n",
    "    type='reddit', \n",
    "    key_word=subreddit.value\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Arxiv data \n",
    "Please run following cell, if you want to change key word run again after make changes in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1701d9e60442119bbd619d557ed1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='machine learning')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arxiv_kw = widgets.Text(value='machine learning')\n",
    "display(arxiv_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit --------------------\n",
      "GETTING ARXIV DATA\n",
      "<modules.arxiv_api.ArxivApi object at 0x1041fb910>\n",
      "[=============================================     ] 90%  Arxiv process"
     ]
    }
   ],
   "source": [
    "# RUN THIS AFTER ARXIV KEYWORD SELECTION\n",
    "collection += setup_process(\n",
    "    type='arxiv', \n",
    "    key_word=arxiv_kw.value\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5300a8431e364dbebd0948ba8180176f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Llama machine learning')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keywords = widgets.Text(value='Llama machine learning')\n",
    "display(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llama', 'machine', 'learning']\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import clean_text_util\n",
    "tokens_kw = clean_text_util(keywords.value)\n",
    "\n",
    "print(tokens_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DATA STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corpus = Corpus()\n",
    "\n",
    "    max_articles = 10\n",
    "    for i in range(max_articles):\n",
    "        doc = collection[i]\n",
    "        corpus.add(author=doc.author , doc=doc)\n",
    "            \n",
    "    vocab = corpus.stats()\n",
    "    vocab = vocab.sort_values('count', ascending=False)\n",
    "                \n",
    "    from program import search_engine\n",
    "    results = search_engine(collection, tokens_kw)\n",
    "    \n",
    "except TypeError as t:\n",
    "    logging.error(t)\n",
    "    raise TypeError\n",
    "except ValueError as v:\n",
    "    logging.error(v)\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document URL: http://arxiv.org/abs/1707.09562v3\n",
      "Similarity Score: 0.638550856814101\n",
      "Document Text: We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/1903.08801v1\n",
      "Similarity Score: 0.5853694070049635\n",
      "Document Text: Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/2312.03120v1\n",
      "Similarity Score: 0.5657789498610037\n",
      "Document Text: With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/2108.07915v1\n",
      "Similarity Score: 0.5486301348591982\n",
      "Document Text: Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/1909.01866v1\n",
      "Similarity Score: 0.531494003452734\n",
      "Document Text: Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/2206.07090v2\n",
      "Similarity Score: 0.5216720300383333\n",
      "Document Text: With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/1907.08908v1\n",
      "Similarity Score: 0.48997894350611143\n",
      "Document Text: Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/1507.02188v1\n",
      "Similarity Score: 0.4803844614152614\n",
      "Document Text: In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/2001.04942v2\n",
      "Similarity Score: 0.41666666666666674\n",
      "Document Text: We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. We discuss a general approach that forms a consistent way to estimate the true underlying machine learning model and demonstrate this in the case of logistic regression.\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zo7or/d_is_there_any_interesting_mathematical_theory_of/\n",
      "Similarity Score: 0.37130532861625287\n",
      "Document Text: Hello everyone! My question is in the title, here is some context. My background can be described as \"major in Theoretical CS (very strong emphasis on the word 'theoretical', think computational complexity theory) with minor in Maths\".  A few years ago I have taken an introductory course on Machine learning and... was severely frustrated and disappointed.  * There was no explanation on how or why anything should work, instead there were lots of unconvincing speculations of the sort like \"if you add a convolution layer, then it will learn simple geometric shapes, so the later layers will have more structure to work with\" or \"we can use an additional input in our RNN and combine the three inputs in a certain way, so the new input will sort of play the role of the 'long-term memory'\". I did not expect a mathematical logic or programming language theory level of rigour, but other sciences, such as economics, at least explain phenomena that they are studying with some *simplified models. Don't we have something similar in machine learning?* * In the course, we jumped straight away to some quite complex problems, like distinguishing pictures of cats from pictures of dogs. I doubt anyone can give a good definition of either class of pictures. While this makes it even more impressive that a neural network can solve the problem, I can't see what we can learn from it about *how* the NN does this. Wouldn't it be better to train the NN to distinguish blue from green, squares from circles etc. and then try use the results to analyse how NNs learn?  Later I've opened a few machine learning textbooks.  * I really liked the parts about PAC learning, and also statistical learning in general were quite my cup of tea. * The parts about neural networks and especially deep learning were almost the same alchemy-level speculations about how ritual dances cause rain that I heard in the course I've taken.  So I tried to find some modern results on arXiv.  * Most papers on machine learning are about to apply even more hard to reason about models to even more difficult to understand problems, which I find really disappointing. * There were some results about neural networks being universal approximators, which is nice even if it is not much. * I've also encountered (while working on my master's thesis) a paper or two about perceptrons and computational complexity of circuits with threshold functions. It is sad that, it seems, there is little to no contemporary research on the topic.  &#x200B;  This concludes my \"kind of rant\", which, I hope, clarifies the type of \"machine learning math\" that I am looking for. Please note that I do understand that working with modern models requires a lot of knowledge and expertise, I am not trying to degrade the work that you do, I am just frustrated with how hard it is to find any theoretical explanations for how these models work, considering the popularity of machine learning.  **I would really appreciate any thoughts or suggestions!**  Also, English is not my first language so I apologise for any typos, incorrect grammar or awkward sentences.  &#x200B;  UPD: saw [this post](https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/?utm_source=share&utm_medium=web2x&context=3) just now. I would really love to see more similar works.\n",
      "\n",
      "Document URL: http://arxiv.org/abs/1212.2686v1\n",
      "Similarity Score: 0.20739033894608508\n",
      "Document Text: We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zz27k/p_llamacpp_gguf_inference_with_a_single_llm/\n",
      "Similarity Score: 0.05634361698190112\n",
      "Document Text: &#x200B;  https://preview.redd.it/i4rxpfwcdtac1.jpg?width=1296&format=pjpg&auto=webp&s=62c2fa0a8d724bfcaa5a21a2e40b7343396bc16f  [txtai](https://github.com/neuml/txtai) has a unified LLM pipeline that can load Hugging Face models, llama.cpp GGUF files and LLM APIs. The example above downloads a GGUF file from the Hugging Face Hub and runs inference with the model.  See this article for more: [https://neuml.hashnode.dev/integrate-llm-frameworks](https://neuml.hashnode.dev/integrate-llm-frameworks)\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zizet/r_hieros_hierarchical_imagination_on_structured/\n",
      "Similarity Score: 0.03021989426077498\n",
      "Document Text: **OpenReview**: [https://openreview.net/forum?id=5j6wtOO6Fk](https://openreview.net/forum?id=5j6wtOO6Fk)  **arXiv**: [https://arxiv.org/abs/2310.05167](https://arxiv.org/abs/2310.05167)  **Code**: [https://github.com/Snagnar/Hieros](https://github.com/Snagnar/Hieros)  **Abstract**:   >One of the biggest challenges to modern deep reinforcement learning   (DRL) algorithms is sample efficiency. Many approaches learn a world   model in order to train an agent entirely in imagination, eliminating   the need for direct environment interaction during training. However,   these methods often suffer from either a lack of imagination accuracy,   exploration capabilities, or runtime efficiency. We propose **Hieros**,  a  hierarchical policy that learns time abstracted world  representations  and imagines trajectories at multiple time scales in  latent space.  Hieros uses an S5 layer-based world model, which predicts  next world  states in parallel during training and iteratively during  environment  interaction. Due to the special properties of S5 layers,  our method can  train in parallel and predict next world states  iteratively during  imagination. This allows for more efficient training  than RNN-based  world models and more efficient imagination than  Transformer-based world  models.    We show that our approach outperforms the state of the art in terms  of  mean and median normalized human score on the Atari 100k benchmark,  and  that our proposed world model is able to predict complex dynamics  very  accurately. We also show that Hieros displays superior exploration   capabilities compared to existing approaches.\n",
      "\n",
      "Document URL: /r/ChatGPT/comments/18zsv1l/google_gemini_potential_training_data_leak/\n",
      "Similarity Score: 0.0\n",
      "Document Text: \n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zgfmx/r_gpt4vision_is_a_generalist_web_agent_if/\n",
      "Similarity Score: 0.0\n",
      "Document Text: Paper: [https://arxiv.org/abs/2401.01614](https://arxiv.org/abs/2401.01614)   Blog: [https://osu-nlp-group.github.io/SeeAct/](https://osu-nlp-group.github.io/SeeAct/)   Code: [https://github.com/OSU-NLP-Group/SeeAct](https://github.com/OSU-NLP-Group/SeeAct)   Abstract:  >The recent development on **large multimodal models (LMMs), especially GPT-4V(ision) and Gemini**, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. **We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites.** This substantially outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2) specifically fine-tuned for web agents. However, grounding still remains a major challenge. Existing LMM grounding strategies like set-of-mark prompting turns out not effective for web agents, and **the best grounding strategy we develop in this paper leverages both the HTML text and visuals.** Yet, there is still a substantial gap with oracle grounding, leaving ample **room for further improvement.**   https://preview.redd.it/1w22ga2ejoac1.jpg?width=706&format=pjpg&auto=webp&s=204d4852c614efaf8c39c990d25a7acae805290e  https://preview.redd.it/vaabea2ejoac1.jpg?width=1344&format=pjpg&auto=webp&s=17f5a5ca7e1add213ca4d75ed53a74e230369655  https://preview.redd.it/2720ob2ejoac1.jpg?width=1340&format=pjpg&auto=webp&s=4cec63cdd3e1448e03f82309ac219684c62b8ffb  https://preview.redd.it/9wn5sa2ejoac1.jpg?width=1242&format=pjpg&auto=webp&s=dcc8919105686007d670f9b140aaeb3e4683d56e  https://preview.redd.it/ttgaad2ejoac1.jpg?width=801&format=pjpg&auto=webp&s=5684aa7969a6564eab8cb4a5ea36fa21f4c63e9e\n",
      "\n",
      "Document URL: /r/LocalLLaMA/comments/18zz33d/seeking_advice_on_fastest_and_highest_quality/\n",
      "Similarity Score: 0.0\n",
      "Document Text: \n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zufv8/r_the_expressive_power_of_transformers_with_chain/\n",
      "Similarity Score: 0.0\n",
      "Document Text: [Paper](https://arxiv.org/abs/2310.07923). I am not affiliated with the authors.  Abstract:  >Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a \"chain of thought\" or \"scratchpad\", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard complexity conjectures): recognizing all regular languages. Our results also imply that linear steps keep transformer decoders within context-sensitive languages, and polynomial steps make them recognize exactly the class of polynomial-time solvable problems -- the first exact characterization of a type of transformers in terms of standard complexity classes. Together, our results provide a nuanced framework for understanding how the length of a transformer's chain of thought or scratchpad impacts its reasoning power.\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/190132l/d_nlp_in_marketing_thesis_ideas/\n",
      "Similarity Score: 0.0\n",
      "Document Text: I am currently enrolled in a MSc in AI and I have to do a thesis related to marketing. My tutor wants me to orient it to NLP but I don´t know what type of projects I could do. It has to involve some model training, not only a LLM-based application.  Most of the stuff I have found online is about sentiment analysis but I would like to consider some other options too.  Thanks!\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/\n",
      "Similarity Score: 0.0\n",
      "Document Text: https://openreview.net/forum?id=tGM7rOmJzV  > (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)\". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.  > ...  > Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks.\n",
      "\n",
      "Document URL: https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/\n",
      "Similarity Score: 0.0\n",
      "Document Text: Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!  Thread will stay alive until next one so keep posting after the date in the title.  Thanks to everyone for answering questions in the previous thread!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"Document URL: {result['Document URL']}\")\n",
    "    print(f\"Similarity Score: {result['Similarity Score']}\")\n",
    "    print(f\"Document Text: {result['Document Text']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lyon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
